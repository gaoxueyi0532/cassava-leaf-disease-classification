import os
import sys
import logging
import random
import json
import time
import math
import numpy as np
import tensorflow as tf
from tensorflow.python.keras.preprocessing.image import ImageDataGenerator
import cv2
import pathlib
from pathlib import Path

feature_dict = {}
feature_dict["image"] = tf.FixedLenFeature([], tf.string)
feature_dict["image_name"] = tf.FixedLenFeature([], tf.string)
feature_dict["target"] = tf.FixedLenFeature([], tf.int64)

''' read and decode serialized data '''
def read_and_decode(filename_queue):
    global feature_dict
    with tf.variable_scope("parse", reuse=True):
        reader = tf.TFRecordReader()
        _, example = reader.read(filename_queue)
        parsed_example = tf.parse_single_example(example, features=feature_dict)

        ''' if parsed_example["image"] generated by call to_bytes(), then call tf.decode_raw(...)'''
        #image = tf.decode_raw(parsed_example["image"], tf.uint8)

        ''' if parsed_example["image"] generated by call cv2.imencode(...), then call tf.image.decode_jpeg '''
        image = tf.image.decode_jpeg(parsed_example["image"])
        image = tf.reshape(image, [384, 384, 3])

        ''' image normalize '''
        image = tf.cast(image, tf.float32) / 255.0

        name = tf.cast(parsed_example["image_name"], tf.string)
        name = tf.reshape(name, [-1])

        label = tf.cast(parsed_example["target"], tf.int32)
        label = tf.reshape(label, [-1])
        return image, name, label

''' generate mini-batch for model training '''
def gen_batch(filename_queue, batch_size, shuffle):
    image, name, label = read_and_decode(filename_queue)
    if shuffle:
        images, names, labels = tf.train.shuffle_batch([image, name, label],\
                                                        batch_size=batch_size,\
                                                        num_threads=6,\
                                                        allow_smaller_final_batch=False, \
                                                        capacity=3000,\
                                                        min_after_dequeue=300)
    else:
        images, names, labels = tf.train.batch([image, name, label], \
                                               batch_size=batch_size,\
                                               num_threads=6,\
                                               capacity=2000)
    return images, names, labels


def batch_norm(inputs, is_training):
    return tf.layers.batch_normalization(inputs, \
                                         epsilon=1e-8, \
                                         momentum=0.99, \
                                         training=is_training)


def conv2D(is_training, \
           inputs, \
           filters, \
           kernal_size=[3,3], \
           padding='valid', \
           strides=1, \
           kernal_initializer=tf.variance_scaling_initializer(), \
           activation=tf.nn.leaky_relu, \
           use_batch_norm=False):
    conv2d_layer = tf.layers.Conv2D(filters=filters,\
                                    kernel_size=kernal_size,\
                                    padding=padding,\
                                    strides=strides,\
                                    kernel_initializer=kernal_initializer)
    outputs = conv2d_layer.apply(inputs)
    if use_batch_norm:
        outputs = batch_norm(outputs, is_training)
    if activation:
        outputs = activation(outputs)
    scope_name = tf.get_variable_scope().name
    conv2d_kernal = tf.get_variable("conv2d/kernel", \
            shape=[kernal_size[0], kernal_size[1], inputs.get_shape()[-1], filters])
    print("%s output shape: %s" % (scope_name, str(outputs.get_shape())))
    return (outputs, conv2d_kernal)


def max_pooling_2D(inputs, \
                   pool_size=[2,2], \
                   strides=2):
    output = tf.layers.max_pooling2d(inputs=inputs, \
                                     pool_size=pool_size, \
                                     strides=strides)
    scope_name = tf.get_variable_scope().name
    print("%s output shape: %s" % (scope_name, str(output.get_shape())))
    shape = output.get_shape()
    size = shape[1] * shape[2] * shape[3]
    return output, size


def dense(is_training, \
          inputs, \
          units, \
          activation=tf.nn.leaky_relu, \
          kernel_initializer=tf.variance_scaling_initializer(), \
          use_batch_norm=False):
    dense_layer = tf.layers.Dense(units=units,\
                                  kernel_initializer=kernel_initializer)
    dense_output = dense_layer.apply(inputs)
    if use_batch_norm:
        dense_output = batch_norm(dense_output, is_training)
    if activation:
        dense_output = activation(dense_output)
    scope_name = tf.get_variable_scope().name
    dense_kernal = tf.get_variable("dense/kernel",
            shape=[inputs.get_shape()[-1], dense_output.get_shape()[-1]])
    print("%s output shape: %s" % (scope_name, str(dense_output.get_shape())))
    return (dense_output, dense_kernal)


def dropout(is_training, inputs, rate):
    output = tf.layers.dropout(inputs=inputs, \
                                rate=rate, \
                                training=is_training)
    scope_name = tf.get_variable_scope().name
    print("%s output shape: %s" % (scope_name, str(output.get_shape())))
    return output


def histogram(scope, inputs):
    return tf.summary.histogram(scope, inputs)


""" build model """
def inference(images, is_training, use_bn=True):
    W_list = []
    A_list = []
    kwargs = {}
    with tf.variable_scope("conv1", reuse=tf.AUTO_REUSE):
        kwargs["inputs"] = images
        kwargs["filters"] = 96
        kwargs["strides"] = 2
        kwargs["kernal_size"] = [3,3]
        kwargs["is_training"] = is_training
        kwargs["use_batch_norm"] = use_bn
        x1, w1 = conv2D(**kwargs)
        W_list.append(histogram("conv1_weights", w1))
        A_list.append(histogram("conv1_activate", x1))

    with tf.variable_scope("conv2", reuse=tf.AUTO_REUSE):
        kwargs["inputs"] = x1
        kwargs["filters"] = 96
        kwargs["strides"] = 1
        kwargs["kernal_size"] = [3,3]
        x2, w2 = conv2D(**kwargs)
        W_list.append(histogram("conv2_weights", w2))
        A_list.append(histogram("conv2_activate", x2))

    with tf.variable_scope("pooL1", reuse=tf.AUTO_REUSE):
        x3, _ = max_pooling_2D(x2)

    with tf.variable_scope("conv3", reuse=tf.AUTO_REUSE):
        kwargs["inputs"] = x3
        kwargs["filters"] = 128
        kwargs["padding"] = 'same'
        x4, w4 = conv2D(**kwargs)
        W_list.append(histogram("conv3_weights", w4))
        A_list.append(histogram("conv3_activate", x4))

    with tf.variable_scope("conv4", reuse=tf.AUTO_REUSE):
        kwargs["inputs"] = x4
        kwargs["filters"] = 128
        x5, w5 = conv2D(**kwargs)
        W_list.append(histogram("conv4_weights", w5))
        A_list.append(histogram("conv4_activate", x5))

    with tf.variable_scope("pooL2", reuse=tf.AUTO_REUSE):
        x6, _ = max_pooling_2D(x5)

    with tf.variable_scope("conv5", reuse=tf.AUTO_REUSE):
        kwargs["inputs"] = x6
        kwargs["filters"] = 256
        x7, w7 = conv2D(**kwargs)
        W_list.append(histogram("conv5_weights", w7))
        A_list.append(histogram("conv5_activate", x7))

    with tf.variable_scope("conv6", reuse=tf.AUTO_REUSE):
        kwargs["inputs"] = x7
        kwargs["strides"] = 2
        kwargs["filters"] = 256
        x8, w8 = conv2D(**kwargs)
        W_list.append(histogram("conv6_weights", w8))
        A_list.append(histogram("conv5_activate", x8))

    with tf.variable_scope("conv7", reuse=tf.AUTO_REUSE):
        kwargs["inputs"] = x8
        kwargs["strides"] = 1
        kwargs["filters"] = 512
        x9, w9 = conv2D(**kwargs)
        W_list.append(histogram("conv7_weights", w9))
        A_list.append(histogram("conv7_activate", x9))

    with tf.variable_scope("conv8", reuse=tf.AUTO_REUSE):
        kwargs["inputs"] = x9
        kwargs["strides"] = 2
        kwargs["filters"] = 512
        x10, w10 = conv2D(**kwargs)
        W_list.append(histogram("conv7_weights", w10))
        A_list.append(histogram("conv7_activate", x10))

    with tf.variable_scope("pooL3", reuse=tf.AUTO_REUSE):
        x11, size = max_pooling_2D(x10)
        x11 = tf.reshape(x11, [-1,size])

    with tf.variable_scope("drop1", reuse=tf.AUTO_REUSE):
        x12 = dropout(is_training, x11, 0.2)

    with tf.variable_scope("fc1", reuse=tf.AUTO_REUSE):
        x13, w13 = dense(is_training, x12, 1024, use_batch_norm=use_bn)
        W_list.append(histogram("fc1_weights", w13))
        A_list.append(histogram("fc1_activate", x13))

    with tf.variable_scope("drop2", reuse=tf.AUTO_REUSE):
        x14 = dropout(is_training, x13, 0.5)

    with tf.variable_scope("fc2", reuse=tf.AUTO_REUSE):
        x15, w15 = dense(is_training, x14, 1024, use_batch_norm=use_bn)
        W_list.append(histogram("fc2_weights", w15))
        A_list.append(histogram("fc2_activate", x15))

    with tf.variable_scope("fc3", reuse=tf.AUTO_REUSE):
        x16, w16 = dense(is_training, \
                         inputs=x15, \
                         units=5, \
                         activation=None, \
                         kernel_initializer=tf.glorot_uniform_initializer())
        W_list.append(histogram("fc3_weights", w16))
        A_list.append(histogram("fc3_activate", x16))

    with tf.variable_scope("softmax", reuse=tf.AUTO_REUSE):
        predict = tf.nn.softmax(x16)
    return x16, predict, W_list, A_list


def get_label_weights(labels, \
                      predicts, \
                      class_num, \
                      alpha, \
                      eta=1.0):
    with tf.variable_scope("label_weights", reuse=True):
        weights = tf.ones(tf.shape(labels), dtype=tf.float32)
        return weights

        # transfer to one-hot format
        onehot_labels = tf.one_hot(labels, class_num)
        onehot_labels = tf.cast(onehot_labels, tf.float32)

        # base Focal-loss: (1 - predict) ** eta
        output = tf.multiply(onehot_labels, predicts)
        predict = tf.reduce_sum(output, 1)
        ones = tf.ones(tf.shape(labels), dtype=tf.float32)
        diff = tf.subtract(ones, predict)
        weights = tf.map_fn(lambda x:tf.pow(x,eta), diff)

        # enforment Focal-loss: put label percent into consider
        class_weights = tf.map_fn(
                lambda x:tf.reduce_sum(tf.multiply(alpha, x)), \
                onehot_labels)
        sample_weights = tf.multiply(class_weights, weights)
        return sample_weights


def compute_loss(logits, labels, label_weights):
    with tf.variable_scope("loss", reuse=True):
        ''' error label existed in dataset, so label smoothing added '''
        cross_entropy = tf.losses.softmax_cross_entropy(onehot_labels=tf.one_hot(labels, 5), \
                                                        logits=logits, \
                                                        weights=label_weights, \
                                                        label_smoothing=0.1)
        #return tf.reduce_mean(cross_entropy)
        return tf.reduce_sum(cross_entropy)


def format_output(names, ys, ys_, predicts):
    print("{0:<20}{1:<10}{2:<10}{3}".format("Name","Label","Predict","Prob"))
    for e in zip(names, ys, ys_, predicts):
        print("{0:<20}{1:<10}{2:<10}{3:.2f}".format(e[0],e[1],e[2],e[3]))


def ndarray_to_list(names, labels, indices, predicts):
    names = [str(e[0]) for e in names.tolist()]
    indices = [e[0] for e in indices.tolist()]
    preds = predicts.tolist()
    preds = [e[1][e[0]] for e in zip(indices, preds)]
    return names, labels.tolist(), indices, preds


def gradient_descent(global_step, decay_steps, loss):
    with tf.variable_scope("backpass", reuse=tf.AUTO_REUSE):
        lr = tf.train.exponential_decay(learning_rate=0.005,\
                                        global_step=global_step,\
                                        decay_steps=decay_steps,\
                                        decay_rate=0.1,\
                                        staircase=True)
        opt = tf.train.AdamOptimizer(learning_rate=lr)
        update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)
        ''' if use batch norm, update_ops need to do before other ops '''
        with tf.control_dependencies(update_ops):
            grad = opt.compute_gradients(loss)
            grads = [(tf.clip_by_value(g,-4,4), v) for i, (g,v) in enumerate(grad)]
            apply_grad_op = opt.apply_gradients(grads, global_step=global_step)
        return apply_grad_op


def get_mini_batch(train_queue, eval_queue, batch_size, is_training):
    with tf.variable_scope("gen_mini_batch", reuse=True):
        train_images, train_names, train_labels = gen_batch(train_queue, batch_size, True)
        eval_images, eval_names, eval_labels = gen_batch(eval_queue, batch_size, False)
        images, names, labels = tf.cond(is_training, \
                                        lambda:(train_images, train_names, train_labels), \
                                        lambda:(eval_images, eval_names, eval_labels))
        labels = tf.reshape(labels, [-1])
        return (images, names, labels)


def aug(image):
    def inner_aug(image):
        angle = tf.random_uniform(shape=[1], minval=0, maxval=1)
        image = tf.contrib.image.rotate(image, angle)
        image = tf.image.random_brightness(image, max_delta=0.2)
        #image = tf.image.random_hue(image, max_delta=0.2)
        image = tf.image.random_contrast(image, lower=0, upper=0.2)
        image = tf.image.random_saturation(image, lower=0, upper=0.1)
        #shape = tf.shape(image)
        #noise = tf.random_normal(shape, mean=0.0, stddev=0.1, dtype=tf.float32)
        #image = image + noise
        image = tf.clip_by_value(image, 0, 1.0)
        return image
    rd = tf.random_uniform(shape=[1], minval=0, maxval=1, dtype=tf.float32)
    eta = tf.constant(0.5, shape=[1], dtype=tf.float32)
    ans = tf.greater(rd[0],eta[0])
    return tf.cond(ans, lambda:image, lambda: inner_aug(image))


def data_augumentation(images):
    with tf.variable_scope("augumentation", reuse=True):
        images = tf.map_fn(lambda x:aug(x), images)
        return images


def resampling(images, names, labels, batch_size):
    labels = labels.tolist()
    images = images.tolist()

    label_to_images = {}
    for i in range(0, batch_size):
        e = labels[i]
        if e in label_to_images:
            label_to_images[e].append(images[i])
        else:
            label_to_images[e] = []
            label_to_images[e].append(images[i])

    ave = batch_size // len(label_to_images)
    c = 0
    for label, image_list in label_to_images.items():
        sz = len(image_list)
        if sz == ave:
            c += sz
        elif sz < ave:
            # over sampling
            image_list.extend(data_augument(image_list, ave-sz))
            label_to_images[label] = image_list
            c += len(image_list)
        else:
            # under sampling
            i = random.randint(0, sz-ave)
            label_to_images[label] = image_list[i : i+ave]
            c += ave

    diff = batch_size - c
    if diff > 0:
        sz, label = min([(len(v),k) for k,v in label_to_images.items()])
        image_list = label_to_images[label]
        image_list.extend(data_augument(image_list, diff))
        label_to_images[label] = image_list

    tuple_list = []
    for label, image_list in label_to_images.items():
        tuple_list.extend([(label,img) for img in image_list])
    del label_to_images

    labels_new = []
    images_new = []
    random.shuffle(tuple_list)
    for label, img in tuple_list:
        labels_new.append(label)
        images_new.append(img)
    del tuple_list
    return np.array(images_new, dtype=np.float32), names, np.array(labels_new, dtype=np.int32)


def compute_accurency(predict, label):
    with tf.variable_scope("acc", reuse=True):
        pred = tf.cast(tf.argmax(predict, 1), dtype=tf.int32)
        correct_pred = tf.equal(pred, label)
        accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))
        return accuracy


def compute_top_k(predict):
    with tf.variable_scope("topk", reuse=True):
        _, indices = tf.nn.top_k(predict)
        return indices

def compute_confusion_matrix(label, predict, class_num=5):
    confusion_matrix = tf.confusion_matrix(label, \
                                           predict, \
                                           num_classes=class_num)
    return confusion_matrix

def Summary(key, value):
    return tf.Summary(value=[tf.Summary.Value(tag=key, simple_value=value),])

def clear_log(train_dir, val_dir):
    train_files = os.listdir(train_dir)
    for tf in train_files: os.remove(os.path.join(train_dir, tf))
    val_files = os.listdir(val_dir)
    for vf in val_files: os.remove(os.path.join(val_dir, vf))

def train():
    root = "/opt/program/services/project/cassava-leaf-disease-classification/"
    model_path = "%s/model/leaf-disease.model" % root

    train_log_dir = root + "src/summary_log/train/"
    val_log_dir = root + "src/summary_log/eval/"
    clear_log(train_log_dir, val_log_dir)

    label_to_disease_dict = {}
    with open(root + "label_num_to_disease_map.json") as f:
        label_to_disease_dict = json.loads(f.readline().strip())

    name_to_label_dict = {}
    per_label_num = {}
    n = 0
    with open(root + "train.csv") as f:
        lst = f.readlines()
        for line in lst:
            n += 1
            name, label = line.strip().split(',')
            name_to_label_dict[name] = int(label)
            key = int(label)
            per_label_num[key] = per_label_num.get(key,0) + 1

    # compute per label ratio
    class_num = len(label_to_disease_dict)
    #label_ratio = [float(per_label_num[i] / n) for i in range(class_num)]
    label_ratio = [1.0 for i in range(class_num)]
    alpha = tf.constant(label_ratio, dtype=tf.float32)

    # read tfrecords data
    tf_records_path = Path('%strain_tfrecords_384_plus_384/' % root)
    all_paths = [str(name) for name in list(tf_records_path.glob('*'))]
    file_num = len(all_paths)

    # data split
    train_paths = all_paths[0:15]
    eval_paths = []
    eval_paths.append(all_paths[-1])

    epoch_num = 30
    batch_size = 32

    train_queue = tf.train.string_input_producer(train_paths)
    eval_queue = tf.train.string_input_producer(eval_paths)

    is_training = tf.placeholder(tf.bool)
    tf.add_to_collection("is_training", is_training)

    # get mini batch
    image, name, label = get_mini_batch(train_queue, eval_queue, batch_size, is_training)

    # data augumention
    #image = data_augumentation(image)

    # define model
    use_bn = True
    logit, predict, W_list, A_list = inference(image, is_training, use_bn)
    W_merge = tf.summary.merge(W_list, name="Weights")
    A_merge = tf.summary.merge(A_list, name="Activations")

    tf.add_to_collection("logit", logit)
    tf.add_to_collection("predict", predict)

    # compute label weights
    label_weights = get_label_weights(label, predict, class_num, alpha)

    # compute loss
    loss = compute_loss(logit, label, label_weights)
    tf.add_to_collection("loss", loss)

    # print all trainal variables
    #vs = tf.trainable_variables()
    #for v in vs: print(v)

    # compute acc
    acc = compute_accurency(predict, label)
    tf.add_to_collection("acc", acc)

    # compute topk
    topk_indices = compute_top_k(predict)
    tf.add_to_collection("top_k_indices", topk_indices)

    # compute confusion matrix
    true_label = tf.placeholder(tf.int32)
    pred_label = tf.placeholder(tf.int32)
    confusion_matrix = compute_confusion_matrix(true_label, pred_label)
    tf.add_to_collection("confusion_matrix", confusion_matrix)
    tf.add_to_collection("true_label", true_label)
    tf.add_to_collection("pred_label", pred_label)

    # backprop
    global_step = tf.train.get_or_create_global_step()
    num_batch_per_epoch = int(len(name_to_label_dict) / batch_size)
    decay_steps = num_batch_per_epoch * 5
    grad_opt = gradient_descent(global_step, decay_steps, loss)

    # summary scala
    loss_scala = tf.summary.scalar('loss', loss)
    acc_scala = tf.summary.scalar('acc', acc)
    merge = tf.summary.merge_all()

    with tf.Session() as sess:
        train_summary_writer = tf.summary.FileWriter("./summary_log/train", graph=sess.graph)
        val_summary_writer = tf.summary.FileWriter("./summary_log/eval", graph=sess.graph)

        saver = tf.train.Saver(allow_empty=True)
        coord = tf.train.Coordinator()
        threads = tf.train.start_queue_runners(sess, coord=coord)

        sess.run(tf.global_variables_initializer())
        sess.run(tf.local_variables_initializer())
        try:
            count = 0
            history_train_loss = []
            history_train_acc = []
            begin = time.time()
            feed = [image, name, label, W_merge, A_merge, predict, logit, label_weights, topk_indices]
            easy_feed = [image, name, label, predict, logit, label_weights]
            feed_map = {}
            for i in range(epoch_num):
                epoch_begin = time.time()
                for j in range(num_batch_per_epoch):
                    count += 1
                    feed_map.clear()
                    feed_map[is_training] = True

                    if count % 20 == 0:
                        _, names, labels, w_merge, a_merge, predicts, logits, weights, indices = sess.run(feed, feed_dict=feed_map)
                    else:
                        _, names, labels, predicts, logits, weights = sess.run(easy_feed, feed_dict=feed_map)

                    # decrease IO opt time
                    if count % 20 == 0:
                        train_summary_writer.add_summary(w_merge, count)
                        train_summary_writer.add_summary(a_merge, count)

                    feed_map[label] = labels
                    feed_map[logit] = logits
                    feed_map[predict] = predicts
                    feed_map[label_weights] = weights
                    train_loss, train_acc = sess.run([loss, acc], feed_dict=feed_map)
                    history_train_loss.append(train_loss)
                    history_train_acc.append(train_acc * batch_size)

                    _ = sess.run(grad_opt, feed_dict={is_training:True, loss:train_loss})

                    # format output
                    if count % 100 == 0:
                        names, labels, indices, predicts = ndarray_to_list(names, labels, indices, predicts)
                        format_output(names, labels, indices, predicts)

                    if count % int(num_batch_per_epoch * 0.1) == 0:
                        train_ave_acc = float(sum(history_train_acc) / (count*batch_size))
                        train_ave_acc_suy = Summary("acc", train_ave_acc)
                        train_summary_writer.add_summary(train_ave_acc_suy, count)
                        train_ave_loss = float(sum(history_train_loss) / (count*batch_size))
                        train_ave_loss_suy = Summary("loss", train_ave_loss)
                        train_summary_writer.add_summary(train_ave_loss_suy, count)
                        print('train step: {}, loss: {}, acc: {}'.format(count, train_ave_loss, train_ave_acc))
                        saver.save(sess, model_path, global_step=count)
                    if count % int(num_batch_per_epoch * 0.1) == 0:
                        val_loss, val_acc = validate(sess, image, name, label, name_to_label_dict)
                        val_loss_suy = Summary("loss", val_loss)
                        val_acc_suy = Summary("acc", val_acc)
                        val_summary_writer.add_summary(val_loss_suy, count)
                        val_summary_writer.add_summary(val_acc_suy, count)
                        print('val step: {}, loss: {}, acc: {}'.format(count, val_loss, val_acc))
                ''' moniting loss on train data '''
                train_ave_acc = float(sum(history_train_acc) / (count*batch_size))
                train_ave_acc_suy = Summary("acc", train_ave_acc)
                train_summary_writer.add_summary(train_ave_acc_suy, count)
                ''' moniting acc on train data '''
                train_ave_loss = float(sum(history_train_loss) / (count*batch_size))
                train_ave_loss_suy = Summary("loss", train_ave_loss)
                train_summary_writer.add_summary(train_ave_loss_suy, count)
                epoch_end = time.time()
                epoch_consume_time = epoch_end - epoch_begin
                print("Epoch %d finish, cousum %d seconds" % (i,epoch_consume_time))
            end = time.time()
            print("consume time: %d" % (end-begin))
        except Exception as e:
            print("train exp: %s" % str(e))
        finally:
            print("finish training")
            train_summary_writer.close()
            val_summary_writer.close()
            coord.request_stop()
            coord.join(threads)


def validate(sess, image, name, label, name_to_label_dict):
    is_training = tf.get_collection("is_training")[0]
    logit = tf.get_collection("logit")[0]
    predict = tf.get_collection("predict")[0]
    acc = tf.get_collection("acc")[0]
    label_weights = tf.placeholder(tf.float32)
    loss = tf.get_collection("loss")[0]
    topk_indices = tf.get_collection("top_k_indices")[0]
    confusion_matrix = tf.get_collection("confusion_matrix")[0]
    true_label = tf.get_collection("true_label")[0]
    pred_label = tf.get_collection("pred_label")[0]

    batch_size = 32
    val_loss = 0.0
    ln = 0.0
    hit = 0.0
    it = 0
    name_set = set()

    all_labels = []
    all_indices = []

    weights = np.array([1.0] * batch_size, dtype=np.float32)
    feed_map = {}
    feed_map[is_training] = False
    feed_map[label_weights] = weights
    feed = [image, name, label, logit, predict, topk_indices, loss, acc]
    while True:
        _, names, labels, _, predicts, indices, batch_val_loss, batch_val_acc = sess.run(feed, feed_dict=feed_map)
        val_loss += batch_val_loss
        for e in zip(names.tolist(), labels.tolist(), indices.tolist()):
            image_name = e[0][0].decode('utf-8')
            if image_name not in name_set:
                name_set.add(image_name)
                hit += (e[1] == e[2][0])
                all_labels.append(e[1])
                all_indices.append(e[2][0])
        print("eval correct num: %d, %f" % (names.size, batch_val_acc * batch_size))
        #names, labels, indices, predicts = ndarray_to_list(names, labels, indices, predicts)
        #format_output(names, labels, indices, predicts)
        it += 1
        ln = len(name_set)
        if ln >= 1330:
            break
        #if ln >= 2670:
        #    break
    feed_map.clear()
    feed_map[true_label] = np.array(all_labels)
    feed_map[pred_label] = np.array(all_indices)
    cf_matrix = sess.run(confusion_matrix, feed_dict=feed_map)
    cf_matrix = cf_matrix.tolist()
    print("confusion matrix:")
    for lst in cf_matrix:
        print("{0:<6}{1:<6}{2:<6}{3:<6}{4}".format(lst[0], lst[1], lst[2], lst[3], lst[4]))
    return float(val_loss/it), float(hit/ln)

if __name__ == '__main__':
    train()
